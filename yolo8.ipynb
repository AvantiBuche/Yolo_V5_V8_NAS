{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "417096f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avant\\my_yolo_v5\\ultralytics\n"
     ]
    }
   ],
   "source": [
    "%cd ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8acfaba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\avant\\\\my_yolo_v5\\\\ultralytics'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47c8b4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\avant\\my_yolo_v5\\ultralytics\\..\\market.jpg: 448x640 7 persons, 2 bowls, 1 apple, 398.1ms\n",
      "Speed: 9.3ms preprocess, 398.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ultralytics.yolo.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.yolo.engine.results.Boxes object\n",
      "keypoints: None\n",
      "keys: ['boxes']\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "orig_img: array([[[239, 250, 218],\n",
      "        [241, 252, 219],\n",
      "        [242, 255, 215],\n",
      "        ...,\n",
      "        [ 85,  20,  11],\n",
      "        [ 85,  20,  11],\n",
      "        [ 84,  19,  10]],\n",
      "\n",
      "       [[243, 253, 223],\n",
      "        [244, 255, 222],\n",
      "        [243, 255, 216],\n",
      "        ...,\n",
      "        [ 92,  30,   0],\n",
      "        [ 92,  30,   0],\n",
      "        [ 92,  30,   0]],\n",
      "\n",
      "       [[244, 254, 224],\n",
      "        [243, 254, 221],\n",
      "        [241, 255, 214],\n",
      "        ...,\n",
      "        [ 82,  45,   0],\n",
      "        [ 82,  45,   0],\n",
      "        [ 83,  46,   0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 87, 116, 155],\n",
      "        [ 98, 127, 166],\n",
      "        [ 97, 126, 163],\n",
      "        ...,\n",
      "        [ 51,  74, 106],\n",
      "        [ 54,  77, 109],\n",
      "        [ 57,  80, 112]],\n",
      "\n",
      "       [[ 75, 106, 145],\n",
      "        [ 91, 122, 161],\n",
      "        [ 88, 120, 156],\n",
      "        ...,\n",
      "        [ 50,  73, 105],\n",
      "        [ 53,  76, 108],\n",
      "        [ 55,  78, 110]],\n",
      "\n",
      "       [[ 70, 101, 140],\n",
      "        [ 94, 125, 164],\n",
      "        [ 96, 128, 164],\n",
      "        ...,\n",
      "        [ 49,  72, 104],\n",
      "        [ 52,  75, 107],\n",
      "        [ 54,  77, 109]]], dtype=uint8)\n",
      "orig_shape: (408, 612)\n",
      "path: 'C:\\\\Users\\\\avant\\\\my_yolo_v5\\\\ultralytics\\\\..\\\\market.jpg'\n",
      "probs: None\n",
      "speed: {'preprocess': 9.276628494262695, 'inference': 398.06461334228516, 'postprocess': 0.0}]\n"
     ]
    }
   ],
   "source": [
    "#!pip install -qr requirements.txt\n",
    "#!pip install ultralytics\n",
    "#!pip install -e ultralytics\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model_test = YOLO('yolov8n.pt')\n",
    "result=model_test.predict(source='../market.jpg',conf=0.25)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77799e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Dependency ultralytics<=8.0.20 is required but found version=8.0.118, to fix: `pip install ultralytics<=8.0.20`\n",
      "Downloading Dataset Version Zip in flowers/Flowers-2 to yolov8: 100% [51085071 / 51085071] bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Dataset Version Zip to flowers/Flowers-2 in yolov8:: 100%|██████████| 1936/1936 [00:09<00:00, 196.10it/s]\n"
     ]
    }
   ],
   "source": [
    "#for custom dataset\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "os.environ[\"DATASET_DIRECTORY\"] = \"flowers\"\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"N3RIafLUvo5XjHHr9CkK\")\n",
    "project = rf.workspace(\"avanti-nlyef\").project(\"flowers-jpdha\")\n",
    "dataset = project.version(2).download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d56823e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\avant\\\\my_yolo_v5\\\\ultralytics\\\\flowers\\\\Flowers-2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "613c36f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.119  Python-3.7.16 torch-1.13.1+cpu CPU\n",
      "YOLOv8s summary (fused): 168 layers, 11156544 parameters, 0 gradients\n",
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\avant\\my_yolo_v5\\ultralytics\\flowers\\Flowers-2\\valid\\labels.cache... 86 images, 0 backgrounds, 0 corrupt: 100%|##########| 86/86 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\avant\\my_yolo_v5\\ultralytics\\flowers\\Flowers-2\\valid\\labels.cache... 86 images, 0 backgrounds, 0 corrupt: 100%|##########| 86/86 [00:00<?, ?it/s]\n",
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/9 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  11%|#1        | 1/9 [00:11<01:33, 11.70s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  22%|##2       | 2/9 [00:21<01:14, 10.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  33%|###3      | 3/9 [00:31<01:01, 10.31s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  44%|####4     | 4/9 [00:42<00:53, 10.70s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  56%|#####5    | 5/9 [00:52<00:41, 10.29s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  67%|######6   | 6/9 [01:02<00:30, 10.10s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  78%|#######7  | 7/9 [01:12<00:20, 10.30s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  89%|########8 | 8/9 [01:21<00:09,  9.89s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|##########| 9/9 [01:28<00:00,  8.93s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|##########| 9/9 [01:28<00:00,  9.85s/it]\n",
      "                   all         86        107      0.435     0.0476      0.069     0.0332\n",
      "                person         86         42      0.304      0.143      0.156     0.0667\n",
      "               bicycle         86         32          1          0     0.0487     0.0318\n",
      "                   car         86         33          0          0    0.00204    0.00102\n",
      "Speed: 5.1ms preprocess, 976.1ms inference, 0.0ms loss, 6.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!yolo task=detect mode=val model=yolov8s.pt data={dataset.location}/data.yaml batch=10 imgsz=640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdefc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to predict objects using webcam\n",
    "#model2 = YOLO('yolov8n.pt')\n",
    "#result2=model2.predict(source='0',show=True)\n",
    "#print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39561dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import supervision as sv\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # to save the video\n",
    "    #writer= cv2.VideoWriter('webcam_yolo.mp4', cv2.VideoWriter_fourcc(*'DIVX'),  7, (1280, 720))\n",
    "    \n",
    "    # define resolution\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 480)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 640)\n",
    "\n",
    "    # specify the model\n",
    "    model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "    # customize the bounding box\n",
    "    box_annotator = sv.BoxAnnotator(\n",
    "        thickness=2,\n",
    "        text_thickness=2,\n",
    "        text_scale=1\n",
    "    )\n",
    "\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        result = model(frame, agnostic_nms=True)[0]\n",
    "        detections = sv.Detections.from_yolov8(result)\n",
    "        labels = [\n",
    "            f\"{model.model.names[class_id]} {confidence:0.2f}\"\n",
    "            for _, confidence, class_id, tracker_id\n",
    "            in detections\n",
    "        ]\n",
    "        frame = box_annotator.annotate(\n",
    "            scene=frame, \n",
    "            detections=detections, \n",
    "            labels=labels\n",
    "        ) \n",
    "        \n",
    "        #writer.write(frame)\n",
    "        \n",
    "        cv2.imshow(\"yolov8\", frame)\n",
    "\n",
    "        if (cv2.waitKey(30) == 27): # break with escape key\n",
    "            break\n",
    "            \n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a44ce8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0baad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bca6589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ecaba61",
   "metadata": {},
   "source": [
    "import cv2\n",
    "import supervision as sv\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # to save the video\n",
    "    #writer= cv2.VideoWriter('webcam_yolo.mp4', cv2.VideoWriter_fourcc(*'DIVX'), 7, (1280, 720))\n",
    "    \n",
    "    # define resolution\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 640)\n",
    "\n",
    "    # specify the model\n",
    "    model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "    # customize the bounding box\n",
    "    box_annotator = sv.BoxAnnotator(\n",
    "        thickness=2,\n",
    "        text_thickness=2,\n",
    "        text_scale=1\n",
    "    )\n",
    "\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        result = model(frame, agnostic_nms=True)[0]\n",
    "        detections = sv.Detections.from_yolov8(result)\n",
    "        labels = [\n",
    "            f\"{model.model.names[class_id]} {confidence:0.2f}\"\n",
    "            for _, confidence, class_id, _\n",
    "            in detections]\n",
    "        \n",
    "        frame = box_annotator.annotate(\n",
    "            scene=frame, \n",
    "            detections=detections, \n",
    "            labels=labels\n",
    "        ) \n",
    "        \n",
    "        #writer.write(frame)\n",
    "        \n",
    "        cv2.imshow(\"yolov8\", frame)\n",
    "\n",
    "        if (cv2.waitKey(30) == 27): # break with escape key\n",
    "            break\n",
    "            \n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2e38de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed93e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6868d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DATASET_DIRECTORY\"] = \"football\"\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"N3RIafLUvo5XjHHr9CkK\")\n",
    "project = rf.workspace(\"bronkscottema\").project(\"football-player-detection\")\n",
    "dataset = project.version(3).download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d302f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0106bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=val model={HOME}/runs/detect/train/weights/best.pt data={dataset.location}/data.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b59c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo predict model=yolov8n.yaml source='https://www.youtube.com/watch?v=ZrLJNazYn8o' imgsz=320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e07670",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo train data=data.yaml model=yolov8n.pt epochs=10 lr0=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ca4653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict a YouTube video using a pretrained segmentation model at image size 320:\n",
    "!yolo predict model=yolov8n-seg.pt source='https://youtu.be/Zgi9g1ksQHc' imgsz=320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd849798",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo detect model={HOME}/runs/detect/train/weights/best.pt data={dataset.location}/data.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af7a84d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f25b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect \\ mode=train \\ model=yolov8s.pt \\ data={dataset.location}/data.yaml \\ epochs=100 \\ imgsz=640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f5e3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect \\ mode=val \\ model={HOME}/runs/detect/train/weights/best.pt \\ data={dataset.location}/data.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa950cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect \\ mode=predict \\ model={HOME}/runs/detect/train/weights/best.pt \\ conf=0.25 \\ source={dataset.location}/test/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b3cad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
